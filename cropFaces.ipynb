{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Face Detection**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Crop the downloaded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "classifier = cv2.CascadeClassifier(r\"haarcascade_frontalface_default.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in os.listdir(r'downloaded_images'):    \n",
    "    frame = cv2.imread(os.path.join(\"downloaded_images\",img))\n",
    "    \n",
    "    face_points = classifier.detectMultiScale(frame,1.3,5)\n",
    "    \n",
    "    if len(face_points)>0:\n",
    "        for x,y,w,h in face_points:\n",
    "            face_frame = frame[y:y+h+1,x:x+w+1]\n",
    "            cv2.imshow(\"Only face\",face_frame)\n",
    "            if len(data)<=100:\n",
    "                data.append(face_frame)\n",
    "                break\n",
    "    # time.sleep(1)\n",
    "    if cv2.waitKey(30) == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "name = input(\"Enter Face holder name : \")\n",
    "for i in range(len(data)):\n",
    "    cv2.imwrite(\"images/\"+name+\"_\"+str(i)+\".jpg\",data[i])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(),'data_dir')\n",
    "img_dir = os.path.join(os.getcwd(),'output')\n",
    "\n",
    "image_data = []\n",
    "labels = []\n",
    "\n",
    "for i in os.listdir(img_dir):\n",
    "    image = cv2.imread(os.path.join(img_dir,i))\n",
    "    image = cv2.resize(image,(100,100))\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    image_data.append(image)\n",
    "    labels.append(str(i).split(\"_\")[0])\n",
    "    \n",
    "image_data = np.array(image_data)    \n",
    "labels = np.array(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir,\"images.p\"),'wb') as f:\n",
    "    pickle.dump(image_data,f)\n",
    "    \n",
    "with open(os.path.join(data_dir,\"labels.p\"),'wb') as f:\n",
    "    pickle.dump(labels,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(r'data_dir/images.p', 'rb') as f:\n",
    "    images = pickle.load(f)\n",
    "\n",
    "with open(r'data_dir/labels.p', 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(390, 100, 100)\n",
      "(390,)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alia', 'deepika', 'jennie', 'srk', 'v'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alia', 'deepika', 'jennie', 'srk', 'v'], dtype='<U7')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_persons = len(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---> alia\n",
      "1 ---> deepika\n",
      "2 ---> jennie\n",
      "3 ---> srk\n",
      "4 ---> v\n"
     ]
    }
   ],
   "source": [
    "l = le.inverse_transform(np.arange(n_persons))\n",
    "for i in range(len(l)):\n",
    "    print(i, \"--->\", l[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocessing(img):\n",
    "    img = cv2.equalizeHist(img)\n",
    "    img = img.reshape(100, 100, 1)\n",
    "    img = img/255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input image:  (390, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "images = np.array(list(map(preprocessing, images)))\n",
    "print(\"Shape of Input image: \", images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lenet_Model():\n",
    "    model = Sequential()\n",
    "    model.add( Conv2D(32, (5,5), input_shape=(100,100,1), activation='relu') )\n",
    "    model.add( MaxPooling2D(pool_size = (2,2)) )\n",
    "\n",
    "    model.add( Conv2D(64, (3,3), activation='relu') )\n",
    "    model.add( MaxPooling2D(pool_size = (2,2)) )\n",
    "\n",
    "\n",
    "    model.add( Flatten() )\n",
    "\n",
    "    model.add( Dense(128, activation='relu') )\n",
    "    model.add( Dense(64, activation='relu') )\n",
    "    model.add( Dense(128, activation='relu') )\n",
    "\n",
    "    model.add( Dense(5, activation='softmax') )\n",
    "    model.compile(Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_58 (Conv2D)          (None, 96, 96, 32)        832       \n",
      "                                                                 \n",
      " max_pooling2d_58 (MaxPooli  (None, 48, 48, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_59 (Conv2D)          (None, 46, 46, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_59 (MaxPooli  (None, 23, 23, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_24 (Flatten)        (None, 33856)             0         \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 128)               4333696   \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4370245 (16.67 MB)\n",
      "Trainable params: 4370245 (16.67 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Lenet_Model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "11/11 [==============================] - 6s 259ms/step - loss: 2.3399 - accuracy: 0.3105 - val_loss: 1.6200 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/15\n",
      "11/11 [==============================] - 2s 225ms/step - loss: 2.0071 - accuracy: 0.3675 - val_loss: 1.3865 - val_accuracy: 0.5385\n",
      "Epoch 3/15\n",
      "11/11 [==============================] - 2s 212ms/step - loss: 1.5443 - accuracy: 0.4587 - val_loss: 0.9466 - val_accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "11/11 [==============================] - 2s 215ms/step - loss: 1.2523 - accuracy: 0.5926 - val_loss: 0.7281 - val_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 1.0561 - accuracy: 0.5954 - val_loss: 0.5484 - val_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "11/11 [==============================] - 2s 217ms/step - loss: 1.0442 - accuracy: 0.6610 - val_loss: 0.3469 - val_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 0.8583 - accuracy: 0.7123 - val_loss: 0.2428 - val_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 0.8487 - accuracy: 0.7037 - val_loss: 0.1461 - val_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 0.6352 - accuracy: 0.7749 - val_loss: 0.0422 - val_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 0.6188 - accuracy: 0.7835 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "11/11 [==============================] - 2s 212ms/step - loss: 0.5387 - accuracy: 0.8034 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 0.5274 - accuracy: 0.8262 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 0.4391 - accuracy: 0.8462 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "11/11 [==============================] - 3s 245ms/step - loss: 0.4044 - accuracy: 0.8604 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 0.3926 - accuracy: 0.8718 - val_loss: 0.0019 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(images, labels, validation_split=0.1, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kalinga\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 423ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(r\"final_model.h5\")\n",
    "\n",
    "classifier = cv2.CascadeClassifier(r'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Enter your ip address\n",
    "URL = 'http://192.168.154.241:8080/shot.jpg' \n",
    "\n",
    "def get_pred_label(pred):\n",
    "    labels = le.inverse_transform([0,1,2,3,4])\n",
    "    return f\"{pred} --> {labels[pred]}\"\n",
    "\n",
    "def preprocess(img):\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img,(100,100))\n",
    "    img = cv2.equalizeHist(img)\n",
    "    img = img.reshape(1,100,100,1)\n",
    "    img = img/255\n",
    "    return img\n",
    "\n",
    "ret = True\n",
    "while ret:\n",
    "    \n",
    "    img_url = urllib.request.urlopen(URL)\n",
    "    image = np.array(bytearray(img_url.read()),np.uint8)\n",
    "    frame = cv2.imdecode(image,-1)\n",
    "    \n",
    "    faces = classifier.detectMultiScale(frame,1.5,5)\n",
    "      \n",
    "    for x,y,w,h in faces:\n",
    "        face = frame[y:y+h,x:x+w]\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,255,0),5)\n",
    "        cv2.putText(\n",
    "            frame,get_pred_label(\n",
    "                np.argmax(model2.predict(preprocess(face)))\n",
    "                ),\n",
    "                (200,200),cv2.FONT_HERSHEY_COMPLEX,1,(255,0,0),2\n",
    "            )\n",
    "        \n",
    "    cv2.imshow(\"capture\",frame)\n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
